{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5hco7zf5lOkfLHEkN5IEy"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Vanishing and Exploding Gradients\n",
        "\n",
        "We just learned how backpropagation through time works and we saw how the gradient of loss can be computed with respect to all the weights in RNN. But here we encounter a problem called the **vanishing and exploding gradients**.\n",
        "\n",
        "While computing derivatives of loss with respect to $W$ and $U$, we saw that we have to traverse all the way back to the first hidden state, as each hidden state at a time $t$ is dependent on its previous hidden state at a time $t - 1$.\n",
        "\n",
        "For instance, say we compute the gradient of loss $L_2$ with respect to $W$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L_2}{\\partial W} = \\frac{\\partial L_2}{\\partial y_2} \\cdot \\frac{\\partial y_2}{\\partial h_2} \\cdot \\frac{\\partial h_2}{\\partial W}\n",
        "$$\n",
        "\n",
        "If you look at the term $\\frac{\\partial h_2}{\\partial W}$ from the above equation, we can’t calculate the derivative of $h_2$ with respect to $W$ directly. Because if you see\n",
        "\n",
        "$$\n",
        "h_2 = \\tanh(Ux_2 + Wh_1)\n",
        "$$\n",
        "\n",
        "it is a function that is dependent on $h_1$ and $W$. So we need to calculate the derivative with respect to $h_1$ as well. But even\n",
        "\n",
        "$$\n",
        "h_1 = \\tanh(Ux_1 + Wh_0)\n",
        "$$\n",
        "\n",
        "is a function that is dependent on $h_0$ and $W$. Thus, we need to calculate the derivative with respect to $h_0$ as well.\n",
        "\n",
        "As shown in the following figure – to compute the derivative of $L_2$, we need to go back all the way to the initial hidden state $h_0$ as each hidden state is dependent on its previous hidden state:\n",
        "\n",
        "![image](https://raw.githubusercontent.com/sudharsan13296/Hands-On-Deep-Learning-Algorithms-with-Python/59efa4d874288f34b07807b848e4dbff34ff8aee/04.%20Generating%20Song%20Lyrics%20Using%20RNN/images/7.png)\n"
      ],
      "metadata": {
        "id": "1IbdyePXfYll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So to compute any loss $L_j$ we need to traverse all the way back to the initial hidden state $h_0$ as each hidden state is dependent on its previous hidden state.\n",
        "\n",
        "Say we have a deep recurrent network with 50 layers. To compute the loss $L_{50}$ we need to traverse all the way back to $h_0$ as shown in the below figure.\n",
        "\n",
        "![image](https://raw.githubusercontent.com/sudharsan13296/Hands-On-Deep-Learning-Algorithms-with-Python/59efa4d874288f34b07807b848e4dbff34ff8aee/04.%20Generating%20Song%20Lyrics%20Using%20RNN/images/8.png)"
      ],
      "metadata": {
        "id": "NQHubBIcgBeD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So exactly what is the problem here? While backpropagating towards the initial hidden state we lose information and the RNN will not backpropagate perfectly.\n",
        "\n",
        "Because remember\n",
        "$$\n",
        " h_t = \\tanh(Ux_t + Wh_{t-1}) \\,\n",
        "$$\n",
        " every time we move backward, we compute the derivative of \\( h_t \\). A derivative of tanh is bounded to 1. We know that any two values between 0 to 1, when multiplied with each other gives us a small number. We usually initialize the weights of the network to a small number. When we multiply the derivatives and weights while backpropagating we are essentially multiplying smaller numbers.\n",
        "\n",
        "So when we multiply smaller numbers at every step while moving backward our gradient becomes infinitesimally small and leads to a number which the computer can't handle and this is called **vanishing gradient problem**.\n",
        "\n",
        "Recall the gradient of the loss with respect of \\( W \\) equation we saw in the previous section:\n",
        "\n",
        "![image](https://raw.githubusercontent.com/sudharsan13296/Hands-On-Deep-Learning-Algorithms-with-Python/59efa4d874288f34b07807b848e4dbff34ff8aee/04.%20Generating%20Song%20Lyrics%20Using%20RNN/images/9.png)\n",
        "As you can see we are multiplying weights and derivative of the tanh function at every time step. Repeated multiplication of these two leads to a small number and causes vanishing gradients problem.\n",
        "\n",
        "Vanishing gradients problem occurs not only in RNN but also in other deep networks where we use sigmoid or tanh as the activation function. So to overcome this we can use ReLu as an activation function instead of tanh. However, we have a variant of the RNN called LSTM network which can solve the vanishing gradient problem effectively. We will see how it works in the next chapter. Similarly, when we initialize the weights of the network to a very large number, gradients would become very large at every step. While backpropagating we multiply a large number together at every time step and it leads to infinity. This is called the __ Exploding Gradient Problem."
      ],
      "metadata": {
        "id": "msz4T9gXguYc"
      }
    }
  ]
}