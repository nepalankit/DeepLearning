{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNVplfmO5sRrjxOtmxtto4c"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rKvGVcZZVYoB"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorboard.plugins import projector\n",
        "\n",
        "# Create a log directory\n",
        "log_dir = 'logs/projector'\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "# Create some dummy data (e.g., 100 vectors of dimension 50)\n",
        "embedding_data = np.random.rand(100, 50).astype(np.float32)\n",
        "\n",
        "# Save the embeddings in a checkpoint file\n",
        "embedding_var = tf.Variable(embedding_data, name='embedding')\n",
        "checkpoint = tf.train.Checkpoint(embedding=embedding_var)\n",
        "checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))\n",
        "\n",
        "# Write a metadata file (optional, but recommended)\n",
        "with open(os.path.join(log_dir, 'metadata.tsv'), 'w') as f:\n",
        "    for i in range(100):\n",
        "        f.write(f\"Label {i}\\n\")\n",
        "\n",
        "# Set up the projector config\n",
        "config = projector.ProjectorConfig()\n",
        "embedding = config.embeddings.add()\n",
        "embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
        "embedding.metadata_path = 'metadata.tsv'\n",
        "\n",
        "# Save the projector config\n",
        "projector.visualize_embeddings(log_dir, config)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "tf.get_logger().setLevel(logging.ERROR)\n"
      ],
      "metadata": {
        "id": "7wiAe_T7WBmk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.contrib.tensorboard.plugins import projector\n",
        "# tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "import numpy as np\n",
        "import gensim\n",
        "import os"
      ],
      "metadata": {
        "id": "iTuHAZ3iWGo0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "file_name = \"/content/sample_data/word2vec.model\"\n",
        "model = gensim.models.keyedvectors.KeyedVectors.load(file_name)\n",
        "\n"
      ],
      "metadata": {
        "id": "e6ZGKUL2WZUi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_size = len(model.wv.index_to_key) - 1"
      ],
      "metadata": {
        "id": "TYeEYGspWtb2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We learned that the dimension of word vectors will be . That is, Length of the vocabulary () Number of neurons in the hidden layer (). So, we initialize a matrix named w2v with the shape as max_size which is the vocabulary size and the model's first layer size which is the number of neurons in the hidden layer:"
      ],
      "metadata": {
        "id": "Lpu0RXGQW0_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v = np.zeros((max_size,model.layer1_size))"
      ],
      "metadata": {
        "id": "GmJlu68lW17U"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Now we create a new file called metadata.tsv where we save all the words in our model and we also store the embedding of each word in the w2v matrix:\n"
      ],
      "metadata": {
        "id": "irEg_wrNW5IM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('projections'):\n",
        "    os.makedirs('projections')\n",
        "\n",
        "with open(\"projections/metadata.tsv\", 'w+') as file_metadata:\n",
        "\n",
        "    # Replace index2word with index_to_key to be compatible with Gensim 4.0.0+\n",
        "    for i, word in enumerate(model.wv.index_to_key[:max_size]):\n",
        "\n",
        "        #store the embeddings of the word\n",
        "        w2v[i] = model.wv[word]\n",
        "\n",
        "        #write the word to a file\n",
        "        file_metadata.write(word + '\\n')"
      ],
      "metadata": {
        "id": "uAwco4zGW66c"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device(\"/cpu:0\"):\n",
        "    embedding = tf.Variable(w2v, trainable=False, name='embedding')\n"
      ],
      "metadata": {
        "id": "WKfCuoBgXBAP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We no longer need tf.train.Saver in TensorFlow 2.x\n",
        "# saver = tf.train.Saver()\n",
        "\n",
        "# Use tf.train.Checkpoint to save the embedding variable\n",
        "checkpoint = tf.train.Checkpoint(embedding=embedding)\n",
        "\n",
        "# Create a directory to save the checkpoint\n",
        "checkpoint_dir = 'projections/checkpoints'\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "# Save the checkpoint\n",
        "checkpoint_path = os.path.join(checkpoint_dir, \"word_embeddings.ckpt\")\n",
        "save_path = checkpoint.save(checkpoint_path)\n",
        "print(f\"Checkpoint saved at: {save_path}\")\n",
        "\n",
        "# Now set up the projector config for the actual word embeddings\n",
        "config = projector.ProjectorConfig()\n",
        "embedding_config = config.embeddings.add()\n",
        "# In TF2, the variable name might be slightly different within the checkpoint\n",
        "# You might need to inspect the checkpoint contents if this path doesn't work\n",
        "embedding_config.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\" # This is a common path for non-Keras variables in TF2 checkpoints\n",
        "embedding_config.metadata_path = 'metadata.tsv'\n",
        "\n",
        "# Save the projector config in the projections directory\n",
        "projector_config_path = os.path.join('projections', 'projector_config.pbtxt')\n",
        "with open(projector_config_path, 'w') as f:\n",
        "    f.write(str(config))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dooV1u7fXLDs",
        "outputId": "22b4833a-f500-4d5a-ee70-8ca3c408bd55"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved at: projections/checkpoints/word_embeddings.ckpt-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "config = projector.ProjectorConfig()\n",
        "embed= config.embeddings.add()\n",
        "\n"
      ],
      "metadata": {
        "id": "x9qppoUtXX-1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed.tensor_name = 'embedding'\n",
        "embed.metadata_path = 'metadata.tsv'"
      ],
      "metadata": {
        "id": "9qmxFPkHXiO8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "projector.visualize_embeddings(log_dir, config)\n",
        "\n"
      ],
      "metadata": {
        "id": "wkmxXjX8XkHc"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}