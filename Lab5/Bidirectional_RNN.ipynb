{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnlpI6MH8IaHvKSdj3vwwd"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## üîÅ BiDirectional RNN\n",
        "\n",
        "In Bidirectional RNN, we have two different layers of hidden units. Both of these layers connect from the input to the output layer. In one layer the hidden states are shared from left to right and in another layer, it is shared from right to left.\n",
        "\n",
        "But what does this mean? To put it simply, one layer moves forward through time from the start of the sequence while another layer moves backward through time from the end of the sequence.\n",
        "\n",
        "As shown in the following figure, we have two hidden layers: forward hidden layer and backward layer.\n",
        "\n",
        "- In the forward hidden layer, hidden state values are shared from past time steps i.e. $h_0$ is shared to $h_1$, $h_1$ is shared to $h_2$ and so on.\n",
        "- In the backward hidden layer, hidden state values are shared from the future time steps i.e. $z_3$ to $z_2$, $z_2$ to $z_1$ and so on.\n",
        "\n",
        "![Bidirectional RNN Diagram](https://camo.githubusercontent.com/f402437b3a0dfc512f6193b62c0669aec210e226e5502d1227f628f0afe1cc54/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f73756468617273616e31333239362f48616e64732d4f6e2d446565702d4c6561726e696e672d416c676f726974686d732d776974682d507974686f6e2f353965666134643837343238386633346230373830376238343865346462666633346666386165652f30342e25323047656e65726174696e67253230536f6e672532304c79726963732532305573696e67253230524e4e2f696d616765732f312e706e67)\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Why Use a Bidirectional RNN?\n",
        "\n",
        "In certain cases, reading the input sequence from both sides is very useful. So a Bidirectional RNN consists of two RNNs, one reading the sentence forward and the other reading the sentence backward.\n",
        "\n",
        "For instance, consider the following sentence:  \n",
        "**Archie lived for 13 years in _____. So he is good at speaking Chinese.**\n",
        "\n",
        "If we use an RNN to predict the blank in the above sentence it would be ambiguous. As we know that RNN can make predictions based only on the set of words it has seen so far. In the above sentence, to predict the blank, RNN has seen only the words, *Archie, lived, for, 13, years, in*. But these words do not provide much context and do not give any clarity to predict the correct word. It just says Archie lived for 13 years in. With this information alone we cannot predict the next word correctly.\n",
        "\n",
        "But if we read the words following the blank as well i.e. *So, he, is, good, at, speaking, Chinese*, then we can say that Archie lived for 13 years in **China** since it is given that he is good at speaking Chinese. So if we use directional RNN to predict the blank, it will predict incorrectly, since it reads the sentence in both forward and backward directions before making predictions.\n",
        "\n",
        "Bidirectional RNN has been used in various applications such as POS tagging where it is vital to know the word before and after the target word, language translation, predicting protein structure, dependency parsing, and so on. However, bidirectional RNN is not suitable for online settings where we don‚Äôt know the future.\n",
        "\n",
        "---\n",
        "\n",
        "### üî¢ Forward Propagation Equations in BiRNN\n",
        "\n",
        "Forward hidden layer:\n",
        "\n",
        "$$\n",
        "h_t = \\sigma(U_h x_t + W_h h_{t-1})\n",
        "$$\n",
        "\n",
        "Backward hidden layer:\n",
        "\n",
        "$$\n",
        "z_t = \\sigma(U_z x_t + W_z z_{t+1})\n",
        "$$\n",
        "\n",
        "Output:\n",
        "\n",
        "$$\n",
        "\\hat{y_t} = \\text{softmax}(V_h h_t + V_z z_t)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Implementing BiDirectional RNN\n",
        "\n",
        "Implementing Bidirectional RNN is simple with TensorFlow.\n",
        "\n",
        "Thus we learned how it works. In the next section, we will see how to implement Bidirectional RNN in TensorFlow.\n"
      ],
      "metadata": {
        "id": "l36_anC-yPjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GhZv5FOGyQlb"
      }
    }
  ]
}